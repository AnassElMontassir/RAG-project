# app.py

import streamlit as st
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.llms import HuggingFaceHub
from langchain.chains import RetrievalQA
import os


#CONFIG
#mod√®le pour transformer les textes en vecteurs (bas√©s sur similarit√© s√©mantique entre les textes <=> similarit√© cosinus entre les vecteurs)
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"

#dossier o√π l‚Äôindex vectoriel est stock√©
INDEX_DIR = "faiss_index"

#mod√®le LLM qu'on utilisera, il est h√©berg√© sur huggingface
HF_MODEL = "mistralai/Mistral-7B-Instruct-v0.1"



#CHARGEMENT DES EMBEDDINGS 
def load_index(index_dir):
    """
    charge l‚Äôindex FAISS √† partir du disque.  
    utilise le mod√®le d'embedding d√©fini dans la config.
    """
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
    return FAISS.load_local(index_dir, embeddings, allow_dangerous_deserialization=True)

def get_qa_chain(vectorstore):
    """
    pr√©pare la cha√Æne de questions/r√©ponses en connectant un LLM
    au syst√®me de r√©cup√©ration de documents (retriever).
    """
    llm = HuggingFaceHub(
        repo_id=HF_MODEL,
        model_kwargs={"temperature": 0.3, "max_new_tokens": 1024},
    )
    retriever = vectorstore.as_retriever()
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True
    )
    return qa_chain

#interface avec streamlit
st.set_page_config(page_title="Book Q&A RAG", layout="wide")
st.title("RAG sur Harry Potter & Hunger Games")

with st.spinner("Chargement de l'index vectoriel..."):
    if not os.path.exists(INDEX_DIR):
        st.error(f"Dossier d'index introuvable : {INDEX_DIR}. Veuillez d'abord ex√©cuter le script d'indexation.")
        st.stop()
    db = load_index(INDEX_DIR)
    qa = get_qa_chain(db)

st.success("Index charg√© ! Pose ta question.")
question = st.text_input("Ta question sur un ou plusieurs livres :")

if question:
    with st.spinner("üß† Recherche de la r√©ponse..."):
        result = qa({"query": question})

        st.subheader("üìå R√©ponse :")
        st.write(result['result'])

        st.subheader("üìö Sources :")
        for doc in result['source_documents']:
            st.markdown(f"**{doc.metadata['source']}** : `{doc.page_content[:200]}...`")
else:
    st.info("Pose une question pour interroger les livres charg√©s par d√©faut.")
